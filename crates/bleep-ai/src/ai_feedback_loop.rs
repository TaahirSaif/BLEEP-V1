/// AI Performance Feedback Loop and Metrics
///
/// This module tracks AI decision accuracy and performance over time.
/// Bad AI becomes progressively less influential through metrics degradation.
///
/// METRICS TRACKED:
/// - Prediction accuracy (vs consensus outcome)
/// - Confidence calibration (confidence vs actual accuracy)
/// - Risk assessment accuracy
/// - False positive rate
/// - Decision latency
/// - Model performance degradation over time
///
/// FEEDBACK MECHANISM:
/// - Low-accuracy AI proposals get lower weight in future epochs
/// - Miscalibrated confidence triggers model retraining
/// - Systematic bias is detected and corrected
/// - Model drift over time triggers governance review

use serde::{Deserialize, Serialize};
use std::collections::{BTreeMap, VecDeque};
use std::fmt;

// ==================== FEEDBACK TYPES ====================

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum FeedbackError {
    InvalidMetric(String),
    InsufficientData(String),
    CalculationError(String),
}

impl fmt::Display for FeedbackError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            Self::InvalidMetric(msg) => write!(f, "Invalid metric: {}", msg),
            Self::InsufficientData(msg) => write!(f, "Insufficient data: {}", msg),
            Self::CalculationError(msg) => write!(f, "Calculation error: {}", msg),
        }
    }
}

impl std::error::Error for FeedbackError {}

// ==================== ACCURACY METRICS ====================

/// Track accuracy of AI predictions
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AccuracyMetrics {
    /// Total predictions evaluated
    pub total_predictions: u64,

    /// Correct predictions
    pub correct_predictions: u64,

    /// Incorrect predictions
    pub incorrect_predictions: u64,

    /// Accuracy rate (0.0-1.0)
    pub accuracy_rate: f32,

    /// True positive rate (0.0-1.0)
    pub true_positive_rate: f32,

    /// False positive rate (0.0-1.0)
    pub false_positive_rate: f32,

    /// True negative rate (0.0-1.0)
    pub true_negative_rate: f32,

    /// False negative rate (0.0-1.0)
    pub false_negative_rate: f32,
}

impl AccuracyMetrics {
    /// Create new metrics
    pub fn new() -> Self {
        Self {
            total_predictions: 0,
            correct_predictions: 0,
            incorrect_predictions: 0,
            accuracy_rate: 0.0,
            true_positive_rate: 0.0,
            false_positive_rate: 0.0,
            true_negative_rate: 0.0,
            false_negative_rate: 0.0,
        }
    }

    /// Record a prediction outcome
    pub fn record_prediction(&mut self, predicted: bool, actual: bool) {
        self.total_predictions += 1;

        if predicted == actual {
            self.correct_predictions += 1;
        } else {
            self.incorrect_predictions += 1;
        }

        // Update rates
        self.recalculate_rates();
    }

    /// Recalculate accuracy rates
    fn recalculate_rates(&mut self) {
        if self.total_predictions == 0 {
            return;
        }

        self.accuracy_rate = self.correct_predictions as f32 / self.total_predictions as f32;

        // True/false rates would need more detailed tracking
        // For now, just use accuracy as indicator
        self.true_positive_rate = self.accuracy_rate * 0.5;
        self.false_positive_rate = (1.0 - self.accuracy_rate) * 0.5;
        self.true_negative_rate = self.accuracy_rate * 0.5;
        self.false_negative_rate = (1.0 - self.accuracy_rate) * 0.5;
    }
}

// ==================== CONFIDENCE CALIBRATION ====================

/// Track whether AI confidence scores match actual outcomes
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ConfidenceCalibration {
    /// Confidence buckets (0.0-0.1, 0.1-0.2, etc.)
    pub buckets: BTreeMap<String, BucketStats>,

    /// Average confidence when correct
    pub avg_confidence_when_correct: f32,

    /// Average confidence when incorrect
    pub avg_confidence_when_incorrect: f32,

    /// Calibration error (lower = better)
    pub calibration_error: f32,

    /// Expected calibration error
    pub expected_calibration_error: f32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BucketStats {
    /// Confidence range
    pub range: (f32, f32),

    /// Predictions in this bucket
    pub predictions: u64,

    /// Correct in this bucket
    pub correct: u64,

    /// Actual accuracy
    pub actual_accuracy: f32,

    /// Expected accuracy (= range midpoint)
    pub expected_accuracy: f32,
}

impl ConfidenceCalibration {
    /// Create new calibration tracker
    pub fn new() -> Self {
        let mut buckets = BTreeMap::new();

        // Create buckets for confidence ranges
        for i in 0..10 {
            let lower = i as f32 * 0.1;
            let upper = (i + 1) as f32 * 0.1;
            let key = format!("{:.1}-{:.1}", lower, upper);

            buckets.insert(
                key,
                BucketStats {
                    range: (lower, upper),
                    predictions: 0,
                    correct: 0,
                    actual_accuracy: 0.0,
                    expected_accuracy: (lower + upper) / 2.0,
                },
            );
        }

        Self {
            buckets,
            avg_confidence_when_correct: 0.0,
            avg_confidence_when_incorrect: 0.0,
            calibration_error: 0.0,
            expected_calibration_error: 0.0,
        }
    }

    /// Record a confidence calibration point
    pub fn record_calibration(&mut self, confidence: f32, was_correct: bool) -> FeedbackResult<()> {
        if !(0.0..=1.0).contains(&confidence) {
            return Err(FeedbackError::InvalidMetric(
                "Confidence must be [0.0, 1.0]".to_string(),
            ));
        }

        // Find bucket
        let bucket_idx = (confidence * 10.0).floor() as usize;
        let bucket_idx = bucket_idx.min(9); // Clamp to valid range

        let lower = bucket_idx as f32 * 0.1;
        let upper = (bucket_idx + 1) as f32 * 0.1;
        let key = format!("{:.1}-{:.1}", lower, upper);

        if let Some(bucket) = self.buckets.get_mut(&key) {
            bucket.predictions += 1;
            if was_correct {
                bucket.correct += 1;
            }
            bucket.actual_accuracy = bucket.correct as f32 / bucket.predictions as f32;
        }

        // Update global stats
        self.recalculate_calibration();

        Ok(())
    }

    /// Recalculate calibration metrics
    fn recalculate_calibration(&mut self) {
        let mut _correct_confidences: Vec<f32> = Vec::new();
        let mut _incorrect_confidences: Vec<f32> = Vec::new();
        let mut total_error = 0.0;
        let mut bucket_count = 0;

        for bucket in self.buckets.values() {
            if bucket.predictions > 0 {
                // Calibration error = |expected - actual|
                let error = (bucket.expected_accuracy - bucket.actual_accuracy).abs();
                total_error += error;
                bucket_count += 1;
            }
        }

        self.calibration_error = if bucket_count > 0 {
            total_error / bucket_count as f32
        } else {
            0.0
        };

        self.expected_calibration_error = self.calibration_error;
    }
}

// ==================== PERFORMANCE TRACKER ====================

/// Track model performance metrics over time
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelPerformance {
    /// Model ID
    pub model_id: String,

    /// Model version
    pub model_version: String,

    /// Accuracy metrics
    pub accuracy: AccuracyMetrics,

    /// Confidence calibration
    pub calibration: ConfidenceCalibration,

    /// Average processing latency (ms)
    pub avg_latency_ms: f32,

    /// Min/max latency
    pub min_latency_ms: f32,
    pub max_latency_ms: f32,

    /// Inferences per epoch
    pub inferences_per_epoch: VecDeque<u64>,

    /// Performance score (0-100, derived from accuracy)
    pub performance_score: f32,

    /// Drift detected (model changing over time)
    pub drift_detected: bool,

    /// Reason for drift (if detected)
    pub drift_reason: Option<String>,

    /// Last recalibration epoch
    pub last_recalibration_epoch: u64,
}

impl ModelPerformance {
    /// Create new performance tracker
    pub fn new(model_id: String, model_version: String) -> Self {
        Self {
            model_id,
            model_version,
            accuracy: AccuracyMetrics::new(),
            calibration: ConfidenceCalibration::new(),
            avg_latency_ms: 0.0,
            min_latency_ms: f32::MAX,
            max_latency_ms: 0.0,
            inferences_per_epoch: VecDeque::new(),
            performance_score: 50.0, // Start neutral
            drift_detected: false,
            drift_reason: None,
            last_recalibration_epoch: 0,
        }
    }

    /// Record inference execution
    pub fn record_inference(&mut self, latency_ms: f32, correct: bool, confidence: f32) -> FeedbackResult<()> {
        // Update latency
        self.avg_latency_ms = (self.avg_latency_ms + latency_ms) / 2.0;
        self.min_latency_ms = self.min_latency_ms.min(latency_ms);
        self.max_latency_ms = self.max_latency_ms.max(latency_ms);

        // Record prediction
        self.accuracy.record_prediction(true, correct); // Simplified
        self.calibration.record_calibration(confidence, correct)?;

        // Update performance score
        self.update_performance_score();

        Ok(())
    }

    /// Update performance score (0-100)
    fn update_performance_score(&mut self) {
        // Score based on accuracy: 0.5 weight accuracy, 0.3 weight calibration, 0.2 weight latency
        let accuracy_score = self.accuracy.accuracy_rate * 100.0;
        let calibration_score = (1.0 - self.calibration.calibration_error) * 100.0;
        let latency_score = if self.avg_latency_ms > 0.0 {
            (1.0 - (self.avg_latency_ms / 1000.0).min(1.0)) * 100.0
        } else {
            50.0
        };

        self.performance_score = (accuracy_score * 0.5 + calibration_score * 0.3 + latency_score * 0.2)
            .clamp(0.0, 100.0);
    }

    /// Detect model drift
    pub fn detect_drift(&mut self, threshold: f32) {
        // Drift = accuracy declining over time
        if self.accuracy.accuracy_rate < threshold && self.accuracy.total_predictions > 100 {
            self.drift_detected = true;
            self.drift_reason = Some(format!(
                "Accuracy below threshold: {:.2}% < {:.2}%",
                self.accuracy.accuracy_rate * 100.0,
                threshold * 100.0
            ));
        }
    }

    /// Record epoch inference count
    pub fn record_epoch_inferences(&mut self, count: u64) {
        self.inferences_per_epoch.push_back(count);
        // Keep only last 100 epochs
        while self.inferences_per_epoch.len() > 100 {
            self.inferences_per_epoch.pop_front();
        }
    }

    /// Get average inferences per epoch (recent)
    pub fn avg_inferences_per_epoch(&self) -> f32 {
        if self.inferences_per_epoch.is_empty() {
            return 0.0;
        }
        let total: u64 = self.inferences_per_epoch.iter().sum();
        total as f32 / self.inferences_per_epoch.len() as f32
    }
}

// ==================== FEEDBACK MANAGER ====================

/// Manages feedback for all models
pub struct FeedbackManager {
    /// Performance per model
    models: BTreeMap<String, ModelPerformance>,

    /// Current epoch
    current_epoch: u64,

    /// Drift detection threshold (default 0.7 = 70% accuracy)
    pub drift_threshold: f32,
}

impl FeedbackManager {
    /// Create new manager
    pub fn new(current_epoch: u64) -> Self {
        Self {
            models: BTreeMap::new(),
            current_epoch,
            drift_threshold: 0.7,
        }
    }

    /// Register a model for tracking
    pub fn register_model(&mut self, model_id: String, model_version: String) {
        let key = format!("{}:{}", model_id, model_version);
        self.models
            .insert(key, ModelPerformance::new(model_id, model_version));
    }

    /// Record inference outcome
    pub fn record_outcome(
        &mut self,
        model_id: &str,
        model_version: &str,
        latency_ms: f32,
        was_correct: bool,
        confidence: f32,
    ) -> FeedbackResult<()> {
        let key = format!("{}:{}", model_id, model_version);

        if let Some(perf) = self.models.get_mut(&key) {
            perf.record_inference(latency_ms, was_correct, confidence)?;
            perf.detect_drift(self.drift_threshold);
        }

        Ok(())
    }

    /// Get model performance
    pub fn get_model_performance(&self, model_id: &str, model_version: &str) -> Option<ModelPerformance> {
        let key = format!("{}:{}", model_id, model_version);
        self.models.get(&key).cloned()
    }

    /// Get all model performance
    pub fn get_all_models(&self) -> Vec<ModelPerformance> {
        self.models.values().cloned().collect()
    }

    /// Get models with drift detected
    pub fn get_drifting_models(&self) -> Vec<ModelPerformance> {
        self.models
            .values()
            .filter(|m| m.drift_detected)
            .cloned()
            .collect()
    }

    /// Calculate aggregate accuracy across all models
    pub fn aggregate_accuracy(&self) -> f32 {
        if self.models.is_empty() {
            return 0.0;
        }

        let total_correct: u64 = self.models.values()
            .map(|m| m.accuracy.correct_predictions)
            .sum();
        let total_predictions: u64 = self.models.values()
            .map(|m| m.accuracy.total_predictions)
            .sum();

        if total_predictions == 0 {
            return 0.0;
        }

        total_correct as f32 / total_predictions as f32
    }

    /// Update epoch
    pub fn update_epoch(&mut self, new_epoch: u64) {
        self.current_epoch = new_epoch;
    }

    /// Get system health metrics
    pub fn get_health_metrics(&self) -> SystemHealthMetrics {
        let total_models = self.models.len();
        let drifting = self.get_drifting_models().len();
        let healthy = total_models - drifting;
        let aggregate_accuracy = self.aggregate_accuracy();

        SystemHealthMetrics {
            total_models,
            healthy_models: healthy,
            drifting_models: drifting,
            aggregate_accuracy,
            overall_health: if drifting == 0 {
                "Healthy".to_string()
            } else {
                "Degraded".to_string()
            },
        }
    }
}

/// System-wide health metrics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SystemHealthMetrics {
    pub total_models: usize,
    pub healthy_models: usize,
    pub drifting_models: usize,
    pub aggregate_accuracy: f32,
    pub overall_health: String,
}

// ==================== TESTS ====================

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_accuracy_metrics() {
        let mut metrics = AccuracyMetrics::new();

        metrics.record_prediction(true, true); // Correct
        metrics.record_prediction(true, false); // Incorrect
        metrics.record_prediction(false, false); // Correct

        assert_eq!(metrics.total_predictions, 3);
        assert_eq!(metrics.correct_predictions, 2);
        assert!((metrics.accuracy_rate - 2.0 / 3.0).abs() < 0.01);
    }

    #[test]
    fn test_confidence_calibration() {
        let mut calibration = ConfidenceCalibration::new();

        // High confidence, correct predictions
        calibration.record_calibration(0.95, true).unwrap();
        calibration.record_calibration(0.95, true).unwrap();

        // Low confidence, incorrect predictions
        calibration.record_calibration(0.15, false).unwrap();

        // Should have some calibration error
        assert!(calibration.calibration_error >= 0.0);
    }

    #[test]
    fn test_model_performance() {
        let mut perf = ModelPerformance::new("test_model".to_string(), "1.0".to_string());

        perf.record_inference(10.0, true, 0.9).unwrap();
        perf.record_inference(15.0, true, 0.85).unwrap();
        perf.record_inference(20.0, false, 0.8).unwrap();

        assert_eq!(perf.accuracy.total_predictions, 3);
        assert!(perf.performance_score > 0.0);
    }

    #[test]
    fn test_feedback_manager() {
        let mut manager = FeedbackManager::new(0);
        manager.register_model("test".to_string(), "1.0".to_string());

        manager.record_outcome("test", "1.0", 10.0, true, 0.9).unwrap();
        manager.record_outcome("test", "1.0", 12.0, true, 0.85).unwrap();

        let perf = manager.get_model_performance("test", "1.0");
        assert!(perf.is_some());

        let health = manager.get_health_metrics();
        assert_eq!(health.total_models, 1);
    }
}
